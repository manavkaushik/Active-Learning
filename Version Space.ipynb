{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q1_b_iii",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CiwABxWA5B-",
        "colab_type": "text"
      },
      "source": [
        "# Active Learning: Q1 (b): iii"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3C6m4lDAhI5",
        "colab_type": "code",
        "outputId": "08b74688-adeb-4c1c-d383-6dd38beb9172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "'''\n",
        "Q1(b)iii\n",
        "What is the size (number of points) of the version space? Order points to label\n",
        "in such a way that the version space gets reduced by maximum with each point\n",
        "chosen to be labelled.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nQ1(b)iii\\nWhat is the size (number of points) of the version space? Order points to label\\nin such a way that the version space gets reduced by maximum with each point\\nchosen to be labelled.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29XblH-KBHGm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "In this section, we shall be calculating the size of the Version space.\n",
        "Moreover, we shall be placing the unlablled points in an order through which labellling the first points in this order results in maximum reduction of version spae.\n",
        "\n",
        "Measure of disagreement used by the committee: **Vote Entropy**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNeaVfhsBGTZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "19a76a95-427f-4f53-a776-3064bcbaf78e"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import math\n",
        "from statistics import mean \n",
        "from sklearn.preprocessing import StandardScaler  \n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn import datasets\n",
        "from scipy.stats import entropy\n",
        "from time import time\n",
        "import seaborn as sns; sns.set()\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGqRNEgxDuYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split dataset into test set, train set and unlabel pool \n",
        "\n",
        "def split(dataset, train_size, test_size): \n",
        "\tx = dataset[:, :-1] \n",
        "\ty = dataset[:, -1] \n",
        "\tx_train, x_pool, y_train, y_pool = train_test_split( \n",
        "\t\tx, y, train_size = train_size) \n",
        "\tunlabel, x_test, label, y_test = train_test_split( \n",
        "\t\tx_pool, y_pool, test_size = test_size) \n",
        "\treturn x_train, y_train, x_test, y_test, unlabel, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFmC_8JSDwRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function defining both the QBC sampling approaches\n",
        "\n",
        "def qbc_sampling(name, unlabel, percent_of_samples, dataset, x_train, y_train):\n",
        "\n",
        "    # We are using BAGGING TECHNIQUE for forming our Committee\n",
        "\n",
        "    #################################################################################\n",
        "    clf1 = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)    \n",
        "    clf1.fit(x_train, y_train)\n",
        "    clf1_pred = clf1.predict(unlabel)\n",
        "\n",
        "    clf2 = BaggingClassifier(base_estimator=SVC(), n_estimators=20, random_state=0)    \n",
        "    clf2.fit(x_train, y_train)\n",
        "    clf2_pred = clf2.predict(unlabel)\n",
        "\n",
        "    clf3 = BaggingClassifier(base_estimator=SVC(), n_estimators=30, random_state=0)    \n",
        "    clf3.fit(x_train, y_train)\n",
        "    clf3_pred = clf3.predict(unlabel)\n",
        "\n",
        "    clf4 = BaggingClassifier(base_estimator=SVC(), n_estimators=40, random_state=0)    \n",
        "    clf4.fit(x_train, y_train)\n",
        "    clf4_pred = clf4.predict(unlabel)\n",
        "\n",
        "    clf5 = BaggingClassifier(base_estimator=SVC(), n_estimators=50, random_state=0)    \n",
        "    clf5.fit(x_train, y_train)\n",
        "    clf5_pred = clf5.predict(unlabel)\n",
        "\n",
        "    # #Classifier 1: Logistic Regression\n",
        "    # clf1 = LogisticRegression() \n",
        "    # x_train, y_train, x_test, y_test, unlabel, label = split( dataset, 0.10, 0.20)\n",
        "    # clf1.fit(x_train, y_train)\n",
        "    # clf1_pred = clf1.predict(unlabel)\n",
        "\n",
        "    # #Classifier 2: RandomForestClassifier\n",
        "    # clf2 = RandomForestClassifier(n_estimators=10) \n",
        "    # x_train, y_train, x_test, y_test, unlabel, label = split( dataset, 0.10, 0.20)\n",
        "    # clf2.fit(x_train, y_train)\n",
        "    # clf2_pred = clf2.predict(unlabel)\n",
        "\n",
        "    # #Classifier 3: AdaBoostClassifier\n",
        "    # clf3 = AdaBoostClassifier(n_estimators=100) \n",
        "    # x_train, y_train, x_test, y_test, unlabel, label = split( dataset, 0.10, 0.20)\n",
        "    # clf3.fit(x_train, y_train)\n",
        "    # clf3_pred = clf3.predict(unlabel)\n",
        "\n",
        "    # #Classifier 4: BaggingClassifier\n",
        "    # clf4 = BaggingClassifier(base_estimator=SVC(), n_estimators=20, random_state=0)    \n",
        "    # x_train, y_train, x_test, y_test, unlabel, label = split( dataset, 0.10, 0.20)\n",
        "    # clf4.fit(x_train, y_train)\n",
        "    # clf4_pred = clf4.predict(unlabel)\n",
        "\n",
        "    # #Classifier 5: GradientBoostingClassifier\n",
        "    # clf5 = GradientBoostingClassifier(random_state=0)\n",
        "    # x_train, y_train, x_test, y_test, unlabel, label = split( dataset, 0.10, 0.20)\n",
        "    # clf5.fit(x_train, y_train)\n",
        "    # clf5_pred = clf5.predict(unlabel)\n",
        "\n",
        "\n",
        "\n",
        "    #################################################################################\n",
        "\n",
        "    if name == 'vote_entropy':\n",
        "        \n",
        "        votes = []\n",
        "        votes_prob = []\n",
        "        for i in range(clf4_pred.shape[0]):\n",
        "            temp = []\n",
        "            temp.append(clf1_pred[i])\n",
        "            temp.append(clf2_pred[i])\n",
        "            temp.append(clf3_pred[i])\n",
        "            temp.append(clf4_pred[i])\n",
        "            temp.append(clf5_pred[i])\n",
        "\n",
        "            votes.append(temp)\n",
        "\n",
        "            temp_p = []\n",
        "            for m in range(10):\n",
        "                temp_p.append(votes[i].count(m) / len(votes[i]))\n",
        "\n",
        "            votes_prob.append(temp_p)\n",
        "\n",
        "        votes = []\n",
        "        votes_prob = []\n",
        "        for i in range(clf4_pred.shape[0]):\n",
        "            temp = []\n",
        "            temp.append(clf1_pred[i])\n",
        "            temp.append(clf2_pred[i])\n",
        "            temp.append(clf3_pred[i])\n",
        "            temp.append(clf4_pred[i])\n",
        "            temp.append(clf5_pred[i])\n",
        "\n",
        "            votes.append(temp)\n",
        "\n",
        "            temp_p = []\n",
        "            for m in range(10):\n",
        "                temp_p.append(votes[i].count(m) / len(votes[i]))\n",
        "\n",
        "            votes_prob.append(temp_p)\n",
        "\n",
        "        from scipy.stats import entropy\n",
        "        entr = entropy(np.array(votes_prob).T)\n",
        "        sorted_idx = np.argsort(entr)\n",
        "        #print('Probability distribution of Top 5 Selection through ENTROPY: \\n')\n",
        "        #for i in range(1,6):\n",
        "            #print(\"Selection #\" + str(i) + ': ' + str(votes[sorted_idx[len(entr) - i]]) + '\\n')\n",
        "        uncrt_pt_ind = [] \n",
        "        for i in range(math.floor(percent_of_samples * 0.01 * dataset.shape[0])):\n",
        "                uncrt_pt_ind.append(sorted_idx[-i-1])\n",
        "\n",
        "        \n",
        "\n",
        "        return uncrt_pt_ind, entr\n",
        "\n",
        "    if name == 'kl_divergence':\n",
        "        proba_votes = []\n",
        "        proba_votes.append(clf1.predict_proba(unlabel))\n",
        "        proba_votes.append(clf2.predict_proba(unlabel))\n",
        "        proba_votes.append(clf3.predict_proba(unlabel))\n",
        "        proba_votes.append(clf4.predict_proba(unlabel))\n",
        "        proba_votes.append(clf5.predict_proba(unlabel))\n",
        "\n",
        "        consensus_proba = (proba_votes[0] + proba_votes[1] + proba_votes[2] + proba_votes[3] + proba_votes[4]) / len(proba_votes)\n",
        "\n",
        "        from scipy.stats import entropy\n",
        "        learner_KL_div = []\n",
        "        for i in range(proba_votes[1].shape[0]):\n",
        "            temp = []\n",
        "            \n",
        "            for j in range(len(proba_votes)):\n",
        "                temp.append(entropy(proba_votes[j][i], qk= consensus_proba[i]))\n",
        "\n",
        "            learner_KL_div.append(temp)\n",
        "\n",
        "        kl_max = np.array(learner_KL_div).max(axis=1)\n",
        "        #print('Size of kl_max= '+str(kl_max.shape))\n",
        "        #print('Size of unlabel= ' + str(unlabel.shape))\n",
        "        sorted_idx = np.argsort(kl_max)\n",
        "        #print('Probability distribution of Top 5 Selection through LEAST CONFIDENCE: \\n')\n",
        "        #for i in range(1,6):\n",
        "            #print(\"Selection #\" + str(i) + ': ' + str(proba[sorted_idx[i-1]]) + '\\n')\n",
        "\n",
        "        uncrt_pt_ind = [] \n",
        "        for i in range(math.floor(percent_of_samples * 0.01 * dataset.shape[0])):\n",
        "            uncrt_pt_ind.append(sorted_idx[-i-1])\n",
        "\n",
        "        return uncrt_pt_ind, learner_KL_div\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krFpegLqDyVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funtion for calculating the version space\n",
        "\n",
        "def calculate_version_space(entropy, threshold, ulabel):\n",
        "\n",
        "    version_space_idx = []\n",
        "    idx = 0\n",
        "    for i in (entropy):\n",
        "        idx = idx+1\n",
        "        if i >= threshold:\n",
        "            version_space_idx.append(idx-1)\n",
        "\n",
        "    print('Size of Version Space: ' + str(len(version_space_idx)) + '  ( ' + str(len(version_space_idx) * 100 / ulabel.shape[0]) + ' % of points)' )\n",
        "    \n",
        "    return version_space_idx\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWyUvFWjHr-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing Data:\n",
        "\n",
        "# Loading dataset\n",
        "X_digits, y_digits = datasets.load_digits(return_X_y=True)\n",
        "stack = np.vstack((X_digits.T, y_digits.T))\n",
        "dataset = stack.T\n",
        "\n",
        "# feature scalling \n",
        "sc = StandardScaler() \n",
        "dataset[:, :-1] = sc.fit_transform(dataset[:, :-1]) \n",
        "\n",
        "# Splitting Data\n",
        "percent_of_samples = 10\n",
        "train_size = 0.10\n",
        "test_size = 0.20\n",
        "\n",
        "x_train, unlabel, y_train, label = train_test_split(X_digits, y_digits, train_size = train_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rittDCCaJ8op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Placing the first 10% points to be labelled in order for maximum reduction in version space!\n",
        "\n",
        "uncrt_pt_ind, entropy = qbc_sampling('vote_entropy', unlabel, percent_of_samples, dataset, x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsAjoiGuLduq",
        "colab_type": "code",
        "outputId": "3e9b8876-273e-4285-d9b0-0b0a762f44ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# CURRENT VERSION SPACE: Before using 10% of the data points with maximum disagreement\n",
        "vs_idx_1 = calculate_version_space(entropy, 0.001, unlabel)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Version Space: 13  ( 0.9034051424600417 % of points)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0ZRRAD2JWVW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Appending the newly labelled 10% data points into the training set\n",
        "x_train = np.append(unlabel[uncrt_pt_ind, :], x_train, axis = 0) \n",
        "#proba_chosen_samples =  classifier1.predict_proba(unlabel[uncrt_pt_ind, :])\n",
        "y_train = np.append(label[uncrt_pt_ind], y_train)\n",
        "\n",
        "# Deleting the newly labelled data points from the unlabelled pool\n",
        "unlabel = np.delete(unlabel, uncrt_pt_ind, axis = 0) \n",
        "label = np.delete(label, uncrt_pt_ind) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgQhubQBKlxQ",
        "colab_type": "code",
        "outputId": "fad77285-385d-45b2-913a-99a75659c832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# NEW VERSION SPACE: After using 10% of the data points with maximum disagreement\n",
        "uncrt_pt_ind, entropy = qbc_sampling('vote_entropy', unlabel, percent_of_samples, dataset, x_train, y_train)\n",
        "vs_idx_2 = calculate_version_space(entropy, 0.01, unlabel)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Version Space: 13  ( 0.9034051424600417 % of points)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnaAZ91JOXz6",
        "colab_type": "code",
        "outputId": "48cdfaa1-9049-4727-f8ce-8bf9252aeed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Initial size of the Version Space: ' + str(len(vs_idx_1)))\n",
        "print('Final size of the Version Space: ' + str(len(vs_idx_2)))\n",
        "print()\n",
        "\n",
        "print('Reduction in Version Space (by labelling 10% of the most disagreed points) = ' + str((len(vs_idx_1) - len(vs_idx_2)) * 100 / len(vs_idx_1)) + ' %')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial size of the Version Space: 33\n",
            "Final size of the Version Space: 13\n",
            "\n",
            "Reduction in Version Space (by labelling 10% of the most disagreed points) = 60.60606060606061 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFDfIxYEQWkg",
        "colab_type": "text"
      },
      "source": [
        "END OF Q1(b)iii"
      ]
    }
  ]
}